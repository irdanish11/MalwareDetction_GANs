# -*- coding: utf-8 -*-
"""
Created on Tue Sep  1 17:28:57 2020

@author: Danish
"""

import tensorflow as tf
from tensorflow.keras.layers import Dense, ReLU, Dropout, BatchNormalization, Input
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from BatchGenerator import BatchGenerator
import numpy as np
from utilities import Timer, info_out, Callbacks, ToJson
from tensorflow.keras.utils import plot_model 
import os

def create_generator(input_shape, name):
    with tf.name_scope(name):
        gen_input = Input(input_shape)
        
        d1 = Dense(input_shape//4)(gen_input)
        lr1 = ReLU()(d1)
        
        d2 = Dense(input_shape//16)(lr1)
        lr2 = ReLU()(d2)
        
        d3 = Dense(input_shape)(lr2)
        lr3 = ReLU()(d3)
        
        generator = Model(inputs=gen_input, outputs=lr3, name=name)
    
    return generator

def create_discriminator(input_shape, opt, name):
    with tf.name_scope(name):
        disc_input = Input(input_shape)
        
        d1 = Dense(input_shape)(disc_input)
        lr1 = ReLU()(d1)
        
        d2 = Dense(512)(lr1)
        lr2 = ReLU()(d2)
        
        # d3 = Dense(256)(lr2)
        # lr3 = ReLU()(d3)
        
        fc = Dense(1, activation='sigmoid')(lr2)
        
        discriminator = Model(inputs=disc_input, outputs=fc, name=name)
        discriminator.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])
    return discriminator

def create_gan(input_shape, opt, name, summary=True):
    generator = create_generator(input_shape, name='Geneartor')
    discriminator = create_discriminator(input_shape, opt, name='Discriminator')
    #Make the discriminator untrainable when we are training the generator.  
    #This doesn't effect the discriminator by itself
    discriminator.trainable = False
    #Combine the two models to create the GAN
    gan_input = Input(shape=input_shape)
    fake_data = generator(gan_input)
    gan_output = discriminator(fake_data)
    
    gan = Model(gan_input, gan_output, name=name)
    #No model compilation because training is done using GradientTape
    gan.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
    if summary:
        generator.summary()
        discriminator.summary()
        gan.summary()
        path='./ModelPlots/'
        os.makedirs(path, exist_ok=True)
        plot_model(generator, to_file=path+'Generator.png', show_shapes=True, dpi=300)
        plot_model(generator, to_file=path+'Discriminator.png', show_shapes=True, dpi=300)
        plot_model(gan, to_file=path+'GAN.png', show_shapes=True, expand_nested=True, dpi=300)
    return generator, discriminator, gan

def train_gan(X, epochs=2, batch_size=64):  
    
    X_goodware = X[0] 
    X_malware = X[1]
    
    #Creating Models
    input_shape = X_malware.shape[1]
    opt = Adam(learning_rate=0.001)
    generator, discriminator, gan = create_gan(input_shape, opt, name='GAN')
    
    #creating callback for saving checkpoints
    cb = Callbacks(ckpt_path='./checkpoints', models=[generator, discriminator, gan])
    
    steps_per_epoch = X_malware.shape[0]//batch_size
    history_epochs = {'Disc_Loss':[], 'Disc_Acc':[], 'Gen_Loss':[], 'Gen_Acc':[], 'Batch_Data':[]}
    
    chk = input('\n\nStart training y/N: ')
    if chk.lower()=='y':
        for epoch in range(1, epochs+1):
            time =Timer()
            history_batch = {'Disc_Loss':[], 'Disc_Acc':[], 'Gen_Loss':[], 'Gen_Acc':[], 'Batch_Data':[]}
            bg = BatchGenerator(X_malware, batch_size, batch_shape=None)
            for batch in range(1, steps_per_epoch+1):
                #start the timer
                time.start()
                #Getting next batch
                batch_malware = bg.get_nextBatch()
                #generating features from Generator
                gen_features = generator.predict(batch_malware)
                #getting samples from goodware for concatenation
                batch_goodware = X_goodware[np.random.randint(0, X_goodware.shape[0], size=batch_size)]
                #converting sparse to dense
                batch_goodware = batch_goodware.todense()
                #Concatenating Goodware and generated features.
                x = np.concatenate((batch_goodware, gen_features))
                #Generating labels for x
                disc_y = np.zeros(2*batch_size)
                disc_y[:batch_size] = 1.0
                
                #Train Discriminator
                disc_metric = discriminator.train_on_batch(x, disc_y)
                history_batch['Disc_Loss'].append(disc_metric[0])
                history_batch['Disc_Acc'].append(disc_metric[1])
                
                #Train Generator using GAN model
                y_gen = np.ones(batch_size)
                gen_metric = gan.train_on_batch(batch_malware, y_gen)
                history_batch['Gen_Loss'].append(gen_metric[0])
                history_batch['Gen_Acc'].append(gen_metric[1])
                
                #Printing info of batch
                time_remain, time_taken = time.get_time_hhmmss(steps_per_epoch-batch)
                timers = (time_remain, time_taken) 
                history = (history_batch, history_epochs)
                info_out('batch', history, timers, epoch, epochs, batch, steps_per_epoch)
                
            #computing loss & accuracy over one epoch
            history_epochs['Disc_Loss'].append(sum(history_batch['Disc_Loss'])/steps_per_epoch)
            history_epochs['Disc_Acc'].append(sum(history_batch['Disc_Acc'])/steps_per_epoch)
            history_epochs['Gen_Loss'].append(sum(history_batch['Gen_Loss'])/steps_per_epoch)
            history_epochs['Gen_Acc'].append(sum(history_batch['Gen_Acc'])/steps_per_epoch)
            history_epochs['Batch_Data'].append(history_batch)
            
            history = (history_batch, history_epochs)
            info_out(which='epoch', history=history, epoch=epoch, total_time=time.get_total_time())
            cb.ckpt_callback(epoch, history_epochs)
    elif chk.lower()=='n':
        SystemExit
    dump(history_epochs, open('./checkpoints/history.obj', 'wb'))
    return history_epochs
            
            
            

            
    